{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, time, json, glob\n",
    "from io import StringIO\n",
    "from datetime import date, timedelta \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import urllib.parse\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import geopandas as gpd\n",
    "\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine, inspect, Column, Integer, Float, String, DateTime\n",
    "from postgis import LineString\n",
    "from postgis.psycopg import register\n",
    "import psycopg2\n",
    "from geoalchemy2 import Geometry, WKTElement\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the instance PostgreSQL to Neon.Tech"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_user = os.environ[\"DATABASE_USER\"]\n",
    "database_password = os.environ[\"DATABASE_PASSWORD\"]\n",
    "database_host = os.environ[\"DATABASE_HOST\"]\n",
    "database_name = os.environ[\"DATABASE_NAME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = f\"postgresql://{database_user}:{database_password}@{database_host}/{database_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the PostgreSQL instance\n",
    "engine = create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = f'{connection_string}'\n",
    "\n",
    "url = urlparse(connection_string)\n",
    "\n",
    "db_host = url.hostname\n",
    "db_port = url.port\n",
    "db_name = url.path[1:]\n",
    "db_user = url.username\n",
    "db_password = url.password"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull from API and write to PostgreSQL using PostGIS formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched data for offset 0\n",
      "Fetched data for offset 2000\n",
      "Fetched data for offset 4000\n",
      "Fetched data for offset 6000\n",
      "Fetched data for offset 8000\n",
      "Fetched data for offset 10000\n",
      "Fetched data for offset 12000\n",
      "Fetched data for offset 14000\n",
      "Fetched data for offset 16000\n",
      "Fetched data for offset 18000\n",
      "Fetched data for offset 20000\n",
      "Fetched data for offset 22000\n",
      "Fetched data for offset 24000\n",
      "Fetched data for offset 26000\n",
      "Fetched data for offset 28000\n",
      "Fetched data for offset 30000\n",
      "Fetched data for offset 32000\n",
      "Fetched data for offset 34000\n",
      "Fetched data for offset 36000\n",
      "Fetched data for offset 38000\n",
      "Fetched data for offset 40000\n",
      "Fetched data for offset 42000\n",
      "Fetched data for offset 44000\n",
      "Fetched data for offset 46000\n",
      "Fetched data for offset 48000\n",
      "Fetched data for offset 50000\n",
      "Fetched data for offset 52000\n",
      "Fetched data for offset 54000\n",
      "Fetched data for offset 56000\n",
      "Fetched data for offset 58000\n",
      "Fetched data for offset 60000\n",
      "Fetched data for offset 62000\n",
      "Fetched data for offset 64000\n",
      "Fetched data for offset 66000\n",
      "Saved final GeoDataFrame with 57450 records to PostgreSQL\n",
      "Total time: 48.25 seconds\n"
     ]
    }
   ],
   "source": [
    "entity = \"tornado_history\"\n",
    "\n",
    "url = r'https://services2.arcgis.com/FiaPA4ga0iQKduv3/arcgis/rest/services/Tornado_Tracks_1950_2017_1/FeatureServer/0/query?'\n",
    "\n",
    "params = {\n",
    "    'where': '1=1',\n",
    "    'geometryType': 'esriGeometryPolygon',\n",
    "    'returnExceededLimitFeatures': 'false',\n",
    "    'inSR': '4326',\n",
    "    'units': 'esriSRUnit_Meter',\n",
    "    'returnGeometry': 'true',\n",
    "    'outFields': '*',\n",
    "    'f': 'pgeojson',\n",
    "    'resultOffset': '0',\n",
    "    'resultRecordCount': '2000'\n",
    "}\n",
    "\n",
    "max_records = 660000\n",
    "\n",
    "count_params = params.copy()\n",
    "count_params['returnCountOnly'] = 'true'\n",
    "count_url = url + urllib.parse.urlencode(count_params)\n",
    "count_response = requests.get(count_url)\n",
    "total_records = count_response.json()['properties']['count']\n",
    "\n",
    "output_directory = f\"/tornado/data/{entity}/\"\n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "def paginate_api(entity, max_records):\n",
    "    num_pages = min(total_records, max_records) // 2000 + (1 if (min(total_records, max_records) % 2000) > 0 else 0)\n",
    "    offsets = [i * 2000 for i in range(num_pages)]\n",
    "    geo_dataframes = []\n",
    "\n",
    "    for offset in offsets:\n",
    "        params['resultOffset'] = str(offset)\n",
    "        url_final = url + urllib.parse.urlencode(params)\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url_final)\n",
    "            data = response.text\n",
    "            df = gpd.read_file(data)\n",
    "            geo_dataframes.append(df)\n",
    "            time.sleep(1)\n",
    "            print(f'Fetched data for offset {offset}')\n",
    "        except Exception as e:\n",
    "            print(f'Error fetching data for offset {offset}: {e}')\n",
    "\n",
    "    final_geo_dataframe = gpd.GeoDataFrame(pd.concat(geo_dataframes, ignore_index=True), crs=df.crs)\n",
    "\n",
    "    engine = create_engine(connection_string)\n",
    "\n",
    "    # Filter out missing geometries before converting to LineString\n",
    "    final_geo_dataframe = final_geo_dataframe[final_geo_dataframe[\"geometry\"].notnull()]\n",
    "    final_geo_dataframe[\"geom\"] = final_geo_dataframe[\"geometry\"].apply(lambda x: LineString(x.coords))\n",
    "\n",
    "    # Write the GeoDataFrame to the database using to_postgis function\n",
    "    final_geo_dataframe.to_postgis(entity, engine, if_exists=\"replace\", index=False)\n",
    "    final_geo_dataframe.to_file(f\"/tornado/data/input/{entity}.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "    print(f'Saved final GeoDataFrame with {len(final_geo_dataframe)} records to PostgreSQL')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    paginate_api(entity, total_records)\n",
    "    print(f'Total time: {time.time() - start_time:.2f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "13930b3feebd826b01a850d818522bf3a9dbd664b32bf0f3d878f211b3671a46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
