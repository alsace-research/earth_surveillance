{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f5c84a-cf1a-4456-9fce-5a7b51d494b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import h3, shapely, us, os\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.parse\n",
    "from sqlalchemy import create_engine  \n",
    "from shapely.ops import unary_union\n",
    "from shapely.geometry import mapping, Polygon\n",
    "import xgboost as xgb\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47f1e6aa",
   "metadata": {},
   "source": [
    "### Prepare US state geo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8385dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', 'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE', 'Florida': 'FL', 'Georgia': 'GA', 'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD', 'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS', 'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH', 'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC', 'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY', 'American Samoa': 'AS', 'Guam': 'GU', 'Northern Mariana Islands': 'MP', 'Puerto Rico': 'PR', 'Virgin Islands': 'VI'}\n"
     ]
    }
   ],
   "source": [
    "def generate_state_abbreviations():\n",
    "    return {state.name: state.abbr for state in us.STATES_AND_TERRITORIES}\n",
    "\n",
    "state_abbreviations = generate_state_abbreviations()\n",
    "print(state_abbreviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99ba285c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n4/p2lmf2l94x3gfqhs72xx2zwr0000gn/T/ipykernel_26811/2996677932.py:3: UserWarning: Geometry is in a geographic CRS. Results from 'area' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  us_states['area'] = us_states.geometry.area\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GEO_ID</th>\n",
       "      <th>STATE</th>\n",
       "      <th>NAME</th>\n",
       "      <th>LSAD</th>\n",
       "      <th>CENSUSAREA</th>\n",
       "      <th>geometry</th>\n",
       "      <th>area</th>\n",
       "      <th>st</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0400000US11</td>\n",
       "      <td>11</td>\n",
       "      <td>District of Columbia</td>\n",
       "      <td></td>\n",
       "      <td>61.048</td>\n",
       "      <td>POLYGON ((-77.03860 38.79151, -77.03890 38.800...</td>\n",
       "      <td>0.018380</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0400000US44</td>\n",
       "      <td>44</td>\n",
       "      <td>Rhode Island</td>\n",
       "      <td></td>\n",
       "      <td>1033.814</td>\n",
       "      <td>MULTIPOLYGON (((-71.38359 41.46478, -71.38928 ...</td>\n",
       "      <td>0.309236</td>\n",
       "      <td>RI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0400000US10</td>\n",
       "      <td>10</td>\n",
       "      <td>Delaware</td>\n",
       "      <td></td>\n",
       "      <td>1948.543</td>\n",
       "      <td>MULTIPOLYGON (((-75.56493 39.58325, -75.57627 ...</td>\n",
       "      <td>0.545358</td>\n",
       "      <td>DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0400000US72</td>\n",
       "      <td>72</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td></td>\n",
       "      <td>3423.775</td>\n",
       "      <td>MULTIPOLYGON (((-65.32770 18.29584, -65.33745 ...</td>\n",
       "      <td>0.765080</td>\n",
       "      <td>PR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0400000US09</td>\n",
       "      <td>09</td>\n",
       "      <td>Connecticut</td>\n",
       "      <td></td>\n",
       "      <td>4842.355</td>\n",
       "      <td>POLYGON ((-71.79924 42.00807, -71.79792 41.935...</td>\n",
       "      <td>1.395161</td>\n",
       "      <td>CT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         GEO_ID STATE                  NAME LSAD  CENSUSAREA  \\\n",
       "8   0400000US11    11  District of Columbia           61.048   \n",
       "39  0400000US44    44          Rhode Island         1033.814   \n",
       "7   0400000US10    10              Delaware         1948.543   \n",
       "51  0400000US72    72           Puerto Rico         3423.775   \n",
       "6   0400000US09    09           Connecticut         4842.355   \n",
       "\n",
       "                                             geometry      area   st  \n",
       "8   POLYGON ((-77.03860 38.79151, -77.03890 38.800...  0.018380  NaN  \n",
       "39  MULTIPOLYGON (((-71.38359 41.46478, -71.38928 ...  0.309236   RI  \n",
       "7   MULTIPOLYGON (((-75.56493 39.58325, -75.57627 ...  0.545358   DE  \n",
       "51  MULTIPOLYGON (((-65.32770 18.29584, -65.33745 ...  0.765080   PR  \n",
       "6   POLYGON ((-71.79924 42.00807, -71.79792 41.935...  1.395161   CT  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load US states GeoDataFrame\n",
    "us_states = gpd.read_file(\"https://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_040_00_5m.json\")\n",
    "us_states['area'] = us_states.geometry.area\n",
    "us_states = us_states.sort_values(by='area', ascending=True)\n",
    "us_states['st'] = us_states['NAME'].map(state_abbreviations)\n",
    "us_states.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b37d8a0",
   "metadata": {},
   "source": [
    "### For each US state, spatial join and aggregate Tornado data into H3 hexagons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34602722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_years_diff(dates):\n",
    "    if len(dates) > 1:\n",
    "        sorted_dates = sorted(dates)\n",
    "        return [(sorted_dates[i + 1] - sorted_dates[i]).days / 365 for i in range(len(sorted_dates) - 1)]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "288c6e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_days_diff(dates):\n",
    "    if len(dates) > 1:\n",
    "        sorted_dates = sorted(dates)\n",
    "        return [(sorted_dates[i + 1] - sorted_dates[i]).days for i in range(len(sorted_dates) - 1)]\n",
    "    else:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ed194ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________\n",
      "Read in geoDataframe for : nan\n",
      "Loading H3 id's and polygons for: nan\n",
      "Error processing nan: single positional indexer is out-of-bounds\n",
      "________________________________________________________________\n",
      "Read in geoDataframe for : RI\n",
      "Loading H3 id's and polygons for: RI\n",
      "We've created the two state objects: full_name and abbreviated name\n",
      "Created event geoDataframe\n",
      "Loading Tornado event layer by the state of: RI\n",
      "Start Join of geoDataframe\n",
      "Resulted join has a record count of: 944\n",
      "Aggregating event geoDataframe\n",
      "Dissolving event geoDataframe\n",
      "Prepare Melt event geoDataframe\n",
      "Successfully dissolved...\n",
      "Wrote to csv...\n",
      "________________________________________________________________\n",
      "Read in geoDataframe for : DE\n",
      "Loading H3 id's and polygons for: DE\n",
      "We've created the two state objects: full_name and abbreviated name\n",
      "Created event geoDataframe\n",
      "Loading Tornado event layer by the state of: DE\n",
      "Start Join of geoDataframe\n",
      "Resulted join has a record count of: 6929\n",
      "Aggregating event geoDataframe\n",
      "Dissolving event geoDataframe\n",
      "Prepare Melt event geoDataframe\n",
      "Successfully dissolved...\n",
      "Wrote to csv...\n",
      "________________________________________________________________\n",
      "Read in geoDataframe for : PR\n",
      "Loading H3 id's and polygons for: PR\n",
      "We've created the two state objects: full_name and abbreviated name\n",
      "Created event geoDataframe\n",
      "Loading Tornado event layer by the state of: PR\n",
      "Start Join of geoDataframe\n",
      "Resulted join has a record count of: 449\n",
      "Aggregating event geoDataframe\n",
      "Dissolving event geoDataframe\n",
      "Prepare Melt event geoDataframe\n",
      "Successfully dissolved...\n",
      "Wrote to csv...\n",
      "________________________________________________________________\n",
      "Read in geoDataframe for : CT\n",
      "Loading H3 id's and polygons for: CT\n",
      "We've created the two state objects: full_name and abbreviated name\n",
      "Created event geoDataframe\n",
      "Loading Tornado event layer by the state of: CT\n",
      "Start Join of geoDataframe\n",
      "Resulted join has a record count of: 10702\n",
      "Aggregating event geoDataframe\n",
      "Dissolving event geoDataframe\n",
      "Prepare Melt event geoDataframe\n",
      "Successfully dissolved...\n",
      "Wrote to csv...\n",
      "________________________________________________________________\n",
      "Read in geoDataframe for : HI\n",
      "Loading H3 id's and polygons for: HI\n",
      "We've created the two state objects: full_name and abbreviated name\n",
      "Created event geoDataframe\n",
      "Loading Tornado event layer by the state of: HI\n",
      "Start Join of geoDataframe\n",
      "Resulted join has a record count of: 1020\n",
      "Aggregating event geoDataframe\n",
      "Dissolving event geoDataframe\n",
      "Prepare Melt event geoDataframe\n",
      "Successfully dissolved...\n",
      "Wrote to csv...\n",
      "________________________________________________________________\n",
      "Read in geoDataframe for : NJ\n",
      "Loading H3 id's and polygons for: NJ\n",
      "We've created the two state objects: full_name and abbreviated name\n",
      "Created event geoDataframe\n",
      "Loading Tornado event layer by the state of: NJ\n",
      "Start Join of geoDataframe\n",
      "Resulted join has a record count of: 17041\n",
      "Aggregating event geoDataframe\n",
      "Dissolving event geoDataframe\n",
      "Prepare Melt event geoDataframe\n",
      "Successfully dissolved...\n",
      "Wrote to csv...\n",
      "________________________________________________________________\n",
      "Read in geoDataframe for : MA\n",
      "Loading H3 id's and polygons for: MA\n",
      "We've created the two state objects: full_name and abbreviated name\n",
      "Created event geoDataframe\n",
      "Loading Tornado event layer by the state of: MA\n",
      "Start Join of geoDataframe\n",
      "Resulted join has a record count of: 16609\n",
      "Aggregating event geoDataframe\n",
      "Dissolving event geoDataframe\n",
      "Prepare Melt event geoDataframe\n",
      "Successfully dissolved...\n",
      "Wrote to csv...\n",
      "________________________________________________________________\n",
      "Read in geoDataframe for : NH\n",
      "Loading H3 id's and polygons for: NH\n",
      "We've created the two state objects: full_name and abbreviated name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function BaseGeometry.__del__ at 0x126fc0700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ /miniforge3/envs/awesome/lib/python3.9/site-packages/shapely/geometry/base.py\", line 209, in __del__\n",
      "    self._empty(val=None)\n",
      "  File \"/Users/ /miniforge3/envs/awesome/lib/python3.9/site-packages/shapely/geometry/base.py\", line 194, in _empty\n",
      "    self._lgeos.GEOSGeom_destroy(self.__geom__)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created event geoDataframe\n",
      "Loading Tornado event layer by the state of: NH\n",
      "Start Join of geoDataframe\n",
      "Resulted join has a record count of: 9619\n",
      "Aggregating event geoDataframe\n",
      "Dissolving event geoDataframe\n",
      "Prepare Melt event geoDataframe\n",
      "Successfully dissolved...\n",
      "Wrote to csv...\n",
      "________________________________________________________________\n",
      "Read in geoDataframe for : VT\n",
      "Loading H3 id's and polygons for: VT\n",
      "We've created the two state objects: full_name and abbreviated name\n",
      "Created event geoDataframe\n",
      "Loading Tornado event layer by the state of: VT\n",
      "Start Join of geoDataframe\n",
      "Resulted join has a record count of: 3453\n",
      "Aggregating event geoDataframe\n",
      "Dissolving event geoDataframe\n",
      "Prepare Melt event geoDataframe\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set directory path\n",
    "directory = \"tornado/data/h3_by_state/\"\n",
    "\n",
    "# Change working directory\n",
    "os.chdir(directory)\n",
    "\n",
    "output_directory = \"h3_hexagons_geopackage\"\n",
    "\n",
    "\n",
    "# Loop through each state name\n",
    "for state_name in us_states['st']:\n",
    "    try:\n",
    "        print(\"________________________________________________________________\")\n",
    "        \n",
    "        # Set directory path\n",
    "        directory = \"tornado/data/h3_by_state/\"\n",
    "\n",
    "        # Change working directory\n",
    "        os.chdir(directory)\n",
    "\n",
    "        # Load H3 by individual state: i.e. AK for Alaska\n",
    "        print(f\"Read in geoDataframe for : {state_name}\")\n",
    "\n",
    "        state = f\"{state_name}\"\n",
    "\n",
    "        # Filter the state records to a single state\n",
    "        st_boundary = us_states[us_states['st'] == f\"{state}\"]\n",
    "        st_boundary = st_boundary.to_crs(4326)\n",
    "\n",
    "        print(f\"Loading H3 id's and polygons for: {state}\")\n",
    "\n",
    "        # Build variable to call the full_name of the state\n",
    "        full_name = st_boundary['NAME'].iloc[0]\n",
    "\n",
    "        print(\"We've created the two state objects: full_name and abbreviated name\")\n",
    "\n",
    "        hexagons = gpd.read_file(f\"{directory}h3_{full_name}.gpkg\")\n",
    "        hexagons['state'] = f\"{state}\"\n",
    "        lookup = hexagons['state'][0]\n",
    "\n",
    "        # Load event data {tornadoes} by individual state\n",
    "        event = gpd.read_file(\"tornado/data/input/tornado_tracks.shp\")\n",
    "        event = event.to_crs(4326)\n",
    "        event_df = event[event['st'] == f\"{lookup}\"]\n",
    "        print(\"Created event geoDataframe\")\n",
    "\n",
    "\n",
    "        # Clip the data using GeoPandas clip\n",
    "        event_df = gpd.clip(event_df, st_boundary)\n",
    "        event_df['buffer_geom'] = event_df.buffer(0.008)\n",
    "        event_df = event_df.drop('geometry', axis=1)\n",
    "        event_df = event_df.rename(columns={'buffer_geom': 'geometry'})\n",
    "        print(f\"Loading Tornado event layer by the state of: {lookup}\")\n",
    "\n",
    "        # Convert column A from float to string and remove \".0\"\n",
    "        event_df['yr'] = event_df['yr'].astype(int).astype(str)\n",
    "        event_df['mo'] = event_df['mo'].astype(int).astype(str)\n",
    "        event_df['dy'] = event_df['dy'].astype(int).astype(str)\n",
    "        #event_df['date'] = pd.to_datetime(event_df['date'], errors='coerce').dt.date\n",
    "        event_df = event_df[['geometry', 'yr', 'mo', 'dy', 'date', 'time', 'tz', 'st', 'mag', 'inj',\n",
    "        'fat', 'loss', 'closs', 'len', 'wid']]\n",
    "        print(\"Start Join of geoDataframe\")\n",
    "\n",
    "        # Use geopandas to spatial join *intersect* the two tables\n",
    "        join_df = gpd.sjoin(hexagons, event_df, how='inner', predicate='intersects')\n",
    "        print(f\"Resulted join has a record count of: {len(join_df.index)}\")\n",
    "\n",
    "        # Sort and group the data to produce aggregate layer\n",
    "        join_df = join_df.sort_values(by=['h3_hexagon', 'date'], ascending=True)\n",
    "        group_df = join_df.groupby('h3_hexagon').agg({\n",
    "            'yr': list, \n",
    "            'mo': list, \n",
    "            'date': list, \n",
    "            'mag': list, \n",
    "            'loss': list, \n",
    "            'closs': list, \n",
    "            'inj': list, \n",
    "            'fat': list, \n",
    "            'len': list, \n",
    "            'wid': list\n",
    "            })\n",
    "        print(\"Aggregating event geoDataframe\")\n",
    "\n",
    "        # Calculate days difference between dates in the 'date' column\n",
    "        #group_df['days_diff'] = group_df['date'].apply(calculate_days_diff)\n",
    "        \n",
    "        #group_df['years_diff'] = group_df['date'].apply(calculate_years_diff)\n",
    "\n",
    "\n",
    "        print(\"Dissolving event geoDataframe\")\n",
    "        # Run a Group By and Sum operation to produce aggregate layer\n",
    "        dissolve_df = join_df.dissolve(\n",
    "                by=\"h3_hexagon\",\n",
    "                aggfunc={'state': 'count',\n",
    "                        'mag':'mean', 'inj': 'sum', \n",
    "                        'fat': 'sum', 'loss': 'sum', \n",
    "                        'closs': 'sum', 'len': 'mean', 'wid': 'mean'})\n",
    "\n",
    "\n",
    "        dissolve_df = dissolve_df.rename(columns={'h3_hexagon':'h3_id', 'name':'tornado_count'})\n",
    "\n",
    "        # Merge the two dataframes to produce final aggregate layer\n",
    "        final = dissolve_df.join(group_df, lsuffix='_h3_history', rsuffix='_stats')\n",
    "        final = final.rename(columns={'state': 'tornado_count'})\n",
    "\n",
    "        df_melt = final.assign(names=final.date.str.split(\",\"))\n",
    "        \n",
    "        print(\"Prepare Melt event geoDataframe\")\n",
    "        final = df_melt.date.apply(pd.Series) \\\n",
    "        .merge(df_melt, right_index=True, left_index=True)\n",
    "\n",
    "        #final_stats = final.drop([0, 1, 'geometry'], axis=1)\n",
    "        final_geom = final.drop([0], axis=1)\n",
    "\n",
    "        #final.to_csv(f\"data/{lookup}_final.csv\")\n",
    "\n",
    "        print(\"Successfully dissolved...\")\n",
    "\n",
    "        # Save to CSV file\n",
    "        final_geom.to_csv(f\"data/{lookup}_final_geom.csv\")\n",
    "        #final_stats.to_csv(f\"data/{lookup}_final_stats.csv\")\n",
    "        print(\"Wrote to csv...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {state_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118bf24d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b5dbdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "108ed504",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'us_states' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m tornado/notebooks/sqlalchemy/03. dissolve_hexagons.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell: tornado/notebooks/sqlalchemy/03.%20dissolve_hexagons.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m output_directory \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mh3_hexagons_geopackage\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell: tornado/notebooks/sqlalchemy/03.%20dissolve_hexagons.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Loop through each state name\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell: tornado/notebooks/sqlalchemy/03.%20dissolve_hexagons.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m state_name \u001b[39min\u001b[39;00m us_states[\u001b[39m'\u001b[39m\u001b[39mst\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell: tornado/notebooks/sqlalchemy/03.%20dissolve_hexagons.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell: tornado/notebooks/sqlalchemy/03.%20dissolve_hexagons.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m________________________________________________________________\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'us_states' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set directory path\n",
    "directory = \"tornado/data/h3_by_state/\"\n",
    "\n",
    "# Change working directory\n",
    "os.chdir(directory)\n",
    "\n",
    "output_directory = \"h3_hexagons_geopackage\"\n",
    "\n",
    "# Loop through each state name\n",
    "for state_name in us_states['st']:\n",
    "    try:\n",
    "        print(\"________________________________________________________________\")\n",
    "\n",
    "        # Set directory path\n",
    "        directory = \"tornado/data/h3_by_state/\"\n",
    "\n",
    "        # Change working directory\n",
    "        os.chdir(directory)\n",
    "\n",
    "        # Load H3 by individual state: i.e. AK for Alaska\n",
    "        print(f\"Read in geoDataframe for : {state_name}\")\n",
    "\n",
    "        state = f\"{state_name}\"\n",
    "\n",
    "        # Filter the state records to a single state\n",
    "        st_boundary = us_states[us_states['st'] == f\"{state}\"]\n",
    "        st_boundary = st_boundary.to_crs(4326)\n",
    "\n",
    "        print(f\"Loading H3 id's and polygons for: {state}\")\n",
    "\n",
    "        # Build variable to call the full_name of the state\n",
    "        full_name = st_boundary['NAME'].iloc[0]\n",
    "\n",
    "        print(\"We've created the two state objects: full_name and abbreviated name\")\n",
    "\n",
    "        hexagons = gpd.read_file(f\"{directory}h3_{full_name}.gpkg\")\n",
    "        hexagons['state'] = f\"{state}\"\n",
    "        lookup = hexagons['state'][0]\n",
    "\n",
    "        # Load event data {tornadoes} by individual state\n",
    "        event = gpd.read_file(\"tornado/data/input/tornado_tracks.shp\")\n",
    "        event = event.to_crs(4326)\n",
    "        event_df = event[event['st'] == f\"{lookup}\"]\n",
    "        print(\"Created event geoDataframe\")\n",
    "\n",
    "        # Clip the data using GeoPandas clip\n",
    "        event_df = gpd.clip(event_df, st_boundary)\n",
    "        event_df['buffer_geom'] = event_df.buffer(0.008)\n",
    "        event_df = event_df.drop('geometry', axis=1)\n",
    "        event_df = event_df.rename(columns={'buffer_geom': 'geometry'})\n",
    "        print(f\"Loading Tornado event layer by the state of: {lookup}\")\n",
    "\n",
    "        # Convert column A from float to string and remove \".0\"\n",
    "        event_df['yr'] = event_df['yr'].astype(int).astype(str)\n",
    "        event_df['mo'] = event_df['mo'].astype(int).astype(str)\n",
    "        event_df['dy'] = event_df['dy'].astype(int).astype(str)\n",
    "        event_df['date'] = pd.to_datetime(event_df['date'], errors='coerce')\n",
    "        event_df = event_df[['geometry', 'yr', 'mo', 'dy', 'date', 'time', 'tz', 'st', 'mag', 'inj',\n",
    "                             'fat', 'loss', 'closs', 'len', 'wid']]\n",
    "        print(\"Start Join of geoDataframe\")\n",
    "\n",
    "        # Use geopandas to spatial join *intersect* the two tables\n",
    "        join_df = gpd.sjoin(hexagons, event_df, how='inner', predicate='intersects')\n",
    "        print(f\"Resulted join has a record count of: {len(join_df.index)}\")\n",
    "\n",
    "        # Sort and group the data to produce aggregate layer\n",
    "        join_df = join_df.sort_values(by=['h3_hexagon', 'date'], ascending=True)\n",
    "        \n",
    "        group_df = join_df.groupby('h3_hexagon').agg({\n",
    "            'yr': list,\n",
    "            'mo': list,\n",
    "            'date': list,\n",
    "            'mag': list,\n",
    "            'loss': list,\n",
    "            'closs': list,\n",
    "            'inj': list,\n",
    "            'fat': list,\n",
    "            'len': list,\n",
    "            'wid': list\n",
    "        })\n",
    "        print(\"Aggregating event geoDataframe\")\n",
    "\n",
    "        # Calculate days difference between dates in the 'date' column\n",
    "        group_df['days_diff'] = group_df['date'].apply(calculate_days_diff)\n",
    "\n",
    "        # Calculate years difference between dates in the 'date' column\n",
    "        group_df['years_diff'] = group_df['date'].apply(calculate_years_diff)\n",
    "\n",
    "        final['years_diff'] = group_df['years_diff']\n",
    "\n",
    "\n",
    "# ... (rest of the code)\n",
    "\n",
    "        # Merge the two dataframes to produce final aggregate layer\n",
    "        final = dissolve_df.join(group_df, lsuffix='_h3_history', rsuffix='_stats')\n",
    "        final = final.rename(columns={'state': 'tornado_count'})\n",
    "\n",
    "        df_melt = final.assign(names=final.date.str.split(\",\"))\n",
    "\n",
    "        print(\"Prepare Melt event geoDataframe\")\n",
    "        final = df_melt.date.apply(pd.Series) \\\n",
    "            .merge(df_melt, right_index=True, left_index=True)\n",
    "\n",
    "        # final_stats = final.drop([0, 1, 'geometry'], axis=1)\n",
    "        final_geom = final.drop([0], axis=1)\n",
    "\n",
    "        print(\"Successfully dissolved...\")\n",
    "\n",
    "        # Save to CSV file\n",
    "        final_geom.to_csv(f\"data/{lookup}_final_geom.csv\")\n",
    "\n",
    "        print(\"Wrote to csv...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {state_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb17260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "13930b3feebd826b01a850d818522bf3a9dbd664b32bf0f3d878f211b3671a46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
